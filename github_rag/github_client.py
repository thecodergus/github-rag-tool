import os
import time
import json
import hashlib
import requests
from typing import Dict, List, Optional, Tuple, Any, Union
from datetime import datetime
import logging

from github_rag.utils import parse_github_repo_url


class GitHubClient:
    """
    Cliente para intera√ß√£o com a API do GitHub com tratamento avan√ßado de limites de taxa,
    cache de requisi√ß√µes e backoff exponencial.

    Otimizado para reposit√≥rios de rob√≥tica como o LeRobot (https://github.com/huggingface/lerobot),
    fornecendo m√©todos espec√≠ficos para an√°lise de issues, PRs e c√≥digo-fonte.
    """

    def __init__(
        self,
        repo_url: str,
        token: Optional[str] = None,
        use_cache: bool = True,
        cache_dir: str = ".github_cache",
        cache_ttl: int = 86400,  # 24 horas em segundos
        log_level: int = logging.INFO,
    ):
        self.repo_url = repo_url
        self.owner, self.repo = parse_github_repo_url(repo_url)
        self.base_url = "https://api.github.com"
        self.headers = {
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "requests",
        }

        # Configura√ß√£o de autentica√ß√£o
        if token:
            self.headers["Authorization"] = f"token {token}"
        else:
            print(
                "‚ö†Ô∏è Operando sem token de autentica√ß√£o. Limites de taxa ser√£o mais restritivos."
            )

        # Configura√ß√£o de cache
        self.use_cache = use_cache
        self.cache_dir = cache_dir
        self.cache_ttl = cache_ttl

        if use_cache and not os.path.exists(cache_dir):
            os.makedirs(cache_dir)

        # Configura√ß√£o de logging
        self.logger = self._setup_logger(log_level)

        # Estat√≠sticas de uso da API
        self.requests_made = 0
        self.cache_hits = 0
        self.rate_limit_hits = 0
        self.last_response = None

        # Informa√ß√µes do reposit√≥rio
        self.repo_info = self._fetch_repo_info()

    def _setup_logger(self, log_level: int) -> logging.Logger:
        """Configura o logger para a classe."""
        logger = logging.getLogger(f"GitHubClient-{self.owner}-{self.repo}")
        logger.setLevel(log_level)

        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)

        return logger

    def _get_cache_key(self, url: str, params: Dict = None) -> str:
        """Gera uma chave de cache √∫nica para uma requisi√ß√£o."""
        params_str = json.dumps(params or {}, sort_keys=True)
        key = f"{url}_{params_str}"
        return hashlib.md5(key.encode()).hexdigest()

    def _get_from_cache(self, cache_key: str) -> Optional[Dict]:
        """Recupera dados do cache se dispon√≠veis e v√°lidos."""
        if not self.use_cache:
            return None

        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")

        if os.path.exists(cache_file):
            # Verificar idade do cache
            cache_age = time.time() - os.path.getmtime(cache_file)
            if cache_age < self.cache_ttl:
                try:
                    with open(cache_file, "r", encoding="utf-8") as f:
                        self.cache_hits += 1
                        self.logger.debug(f"Cache hit para {cache_key}")
                        return json.load(f)
                except Exception as e:
                    self.logger.warning(f"Erro ao ler cache: {str(e)}")
            else:
                self.logger.debug(
                    f"Cache expirado para {cache_key} (idade: {cache_age:.1f}s)"
                )

        return None

    def _save_to_cache(self, cache_key: str, data: Dict) -> None:
        """Salva dados no cache."""
        if not self.use_cache:
            return

        cache_file = os.path.join(self.cache_dir, f"{cache_key}.json")

        try:
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False)
                self.logger.debug(f"Dados salvos em cache: {cache_key}")
        except Exception as e:
            self.logger.warning(f"Erro ao salvar cache: {str(e)}")

    def check_rate_limit(self) -> Tuple[Optional[int], Optional[int]]:
        """
        Verifica os limites de taxa atuais da API.

        Returns:
            Tuple[int, int]: (requisi√ß√µes restantes, timestamp para reset)
        """
        url = f"{self.base_url}/rate_limit"

        try:
            response = requests.get(url, headers=self.headers)
            self.last_response = response

            if response.status_code == 200:
                data = response.json()

                # Obter limites de taxa core e search
                core_rate = data["resources"]["core"]
                search_rate = data["resources"]["search"]

                # Formatar hor√°rio de reset para exibi√ß√£o
                core_reset_time = datetime.fromtimestamp(core_rate["reset"]).strftime(
                    "%H:%M:%S"
                )
                search_reset_time = datetime.fromtimestamp(
                    search_rate["reset"]
                ).strftime("%H:%M:%S")

                # Informar limites de taxa
                self.logger.info("\n--- Limites de Taxa da API GitHub ---")
                self.logger.info(
                    f"üìä Core API: {core_rate['remaining']}/{core_rate['limit']} restantes"
                )
                self.logger.info(
                    f"üîé Search API: {search_rate['remaining']}/{search_rate['limit']} restantes"
                )
                self.logger.info(f"‚è±Ô∏è Core API reset √†s: {core_reset_time}")
                self.logger.info(f"‚è±Ô∏è Search API reset √†s: {search_reset_time}")

                # Avisar se estiver pr√≥ximo do limite
                if core_rate["remaining"] < (core_rate["limit"] * 0.1):
                    self.logger.warning(
                        f"‚ö†Ô∏è ATEN√á√ÉO: Menos de 10% das requisi√ß√µes Core dispon√≠veis!"
                    )

                return core_rate["remaining"], core_rate["reset"]
            else:
                self.logger.error(
                    f"‚ùå Erro ao verificar limites de taxa: {response.status_code}"
                )
                return None, None

        except Exception as e:
            self.logger.error(f"‚ùå Exce√ß√£o ao verificar limites de taxa: {str(e)}")
            return None, None

    def _make_request(
        self,
        url: str,
        params: Optional[Dict] = None,
        method: str = "GET",
        data: Optional[Dict] = None,
        error_message: str = "Erro na requisi√ß√£o",
        max_retries: int = 5,
        use_cache: Optional[bool] = None,
    ) -> Optional[Dict]:
        """
        Realiza uma requisi√ß√£o HTTP com tratamento avan√ßado de erros e limites de taxa.

        Args:
            url: URL completa da requisi√ß√£o
            params: Par√¢metros de query string
            method: M√©todo HTTP (GET, POST, etc)
            data: Dados para enviar (para POST, PUT, etc)
            error_message: Mensagem personalizada para erros
            max_retries: N√∫mero m√°ximo de tentativas
            use_cache: Sobrescreve configura√ß√£o global de cache

        Returns:
            Dict: Resposta da API em formato JSON ou None em caso de erro
        """
        use_cache = self.use_cache if use_cache is None else use_cache

        # Gerar chave de cache e verificar se temos dados em cache
        if method == "GET" and use_cache:
            cache_key = self._get_cache_key(url, params)
            cached_data = self._get_from_cache(cache_key)

            if cached_data:
                self.logger.debug(f"üîÑ Usando dados em cache para: {url}")
                return cached_data

        # Fazer a requisi√ß√£o com retentativas
        retries = 0
        self.requests_made += 1

        while retries < max_retries:
            try:
                if method == "GET":
                    response = requests.get(
                        url, headers=self.headers, params=params, timeout=30
                    )
                elif method == "POST":
                    response = requests.post(
                        url, headers=self.headers, params=params, json=data, timeout=30
                    )
                elif method == "PUT":
                    response = requests.put(
                        url, headers=self.headers, params=params, json=data, timeout=30
                    )
                elif method == "DELETE":
                    response = requests.delete(
                        url, headers=self.headers, params=params, timeout=30
                    )
                else:
                    raise ValueError(f"M√©todo HTTP n√£o suportado: {method}")

                # Salvar a √∫ltima resposta para uso em outros m√©todos
                self.last_response = response

                # Extrair informa√ß√µes de limite de taxa dos cabe√ßalhos
                remaining = int(response.headers.get("X-RateLimit-Remaining", 0))
                limit = int(response.headers.get("X-RateLimit-Limit", 0))
                reset_time = int(response.headers.get("X-RateLimit-Reset", 0))

                # Se for bem-sucedido, retornar dados e cachear
                if response.status_code == 200 or response.status_code == 201:
                    result = response.json()

                    # Salvar em cache se for GET
                    if method == "GET" and use_cache:
                        self._save_to_cache(cache_key, result)

                    # Avisar se estiver com poucas requisi√ß√µes restantes
                    if remaining < (limit * 0.1) and limit > 0:
                        reset_datetime = datetime.fromtimestamp(reset_time).strftime(
                            "%H:%M:%S"
                        )
                        self.logger.warning(
                            f"‚ö†Ô∏è Apenas {remaining}/{limit} requisi√ß√µes restantes at√© {reset_datetime}"
                        )

                    return result

                # Tratar limites de taxa (403/429)
                elif response.status_code in (403, 429):
                    self.rate_limit_hits += 1

                    # Verificar se √© realmente um problema de limite de taxa
                    if (
                        "X-RateLimit-Remaining" in response.headers
                        and int(response.headers["X-RateLimit-Remaining"]) == 0
                    ):
                        reset_time = int(response.headers.get("X-RateLimit-Reset", 0))
                        current_time = time.time()
                        sleep_time = (
                            max(reset_time - current_time, 0) + 2
                        )  # Margem de seguran√ßa

                        self.logger.warning(
                            f"‚è≥ Limite de taxa atingido. Aguardando {sleep_time:.1f} segundos at√© reset..."
                        )
                        time.sleep(sleep_time)
                        retries += 1
                        continue

                    # Se for 429, usar o header Retry-After se dispon√≠vel
                    if (
                        response.status_code == 429
                        and "Retry-After" in response.headers
                    ):
                        retry_after = int(response.headers["Retry-After"])
                        self.logger.warning(
                            f"‚è≥ Taxa excedida. Aguardando {retry_after} segundos conforme solicitado."
                        )
                        time.sleep(retry_after)
                        retries += 1
                        continue

                    # Backoff exponencial para outras tentativas
                    wait_time = (2**retries) + (
                        time.time() % 1
                    )  # Adiciona um pouco de aleatoriedade
                    self.logger.warning(
                        f"‚è≥ {error_message} ({response.status_code}). Tentativa {retries+1}/{max_retries} em {wait_time:.1f}s"
                    )
                    time.sleep(wait_time)
                    retries += 1

                # Outros erros
                else:
                    error_data = response.json() if response.text else {}
                    error_msg = error_data.get("message", "Sem detalhes do erro")
                    self.logger.error(
                        f"‚ùå {error_message}: {response.status_code} - {error_msg}"
                    )

                    # Alguns erros n√£o devem ser retentados
                    if response.status_code in (401, 404, 422):
                        return None

                    # Para outros erros, tentar novamente com backoff
                    wait_time = (2**retries) + (time.time() % 1)
                    self.logger.warning(
                        f"‚è≥ Tentativa {retries+1}/{max_retries} em {wait_time:.1f}s"
                    )
                    time.sleep(wait_time)
                    retries += 1

            except requests.exceptions.RequestException as e:
                self.logger.error(f"‚ùå Erro de conex√£o: {str(e)}")
                wait_time = (2**retries) + (time.time() % 1)
                self.logger.warning(
                    f"‚è≥ Tentativa {retries+1}/{max_retries} em {wait_time:.1f}s"
                )
                time.sleep(wait_time)
                retries += 1

        # Se chegou aqui, todas as tentativas falharam
        self.logger.error(f"‚ùå Falha ap√≥s {max_retries} tentativas para: {url}")
        return None

    def _fetch_repo_info(self) -> Optional[Dict]:
        """
        Busca informa√ß√µes b√°sicas sobre o reposit√≥rio.

        Returns:
            Dict: Informa√ß√µes do reposit√≥rio ou None em caso de erro
        """
        url = f"{self.base_url}/repos/{self.owner}/{self.repo}"
        return self._make_request(
            url, error_message="Erro ao obter informa√ß√µes do reposit√≥rio"
        )

    def fetch_issues(
        self,
        state: str = "all",
        per_page: int = 100,
        since: Optional[str] = None,
        labels: Optional[str] = None,
        sort: str = "created",
        direction: str = "desc",
    ) -> List[Dict]:
        """
        Obt√©m todas as issues de um reposit√≥rio, lidando com pagina√ß√£o.

        Args:
            state: Estado das issues (open, closed, all)
            per_page: N√∫mero de itens por p√°gina
            since: Data ISO 8601 (YYYY-MM-DDTHH:MM:SSZ) para filtrar issues atualizadas ap√≥s esta data
            labels: Lista de labels separadas por v√≠rgula
            sort: Campo para ordena√ß√£o (created, updated, comments)
            direction: Dire√ß√£o da ordena√ß√£o (asc, desc)

        Returns:
            List[Dict]: Lista com todas as issues
        """
        url = f"{self.base_url}/repos/{self.owner}/{self.repo}/issues"
        params = {
            "state": state,
            "per_page": per_page,
            "page": 1,
            "sort": sort,
            "direction": direction,
        }

        if since:
            params["since"] = since

        if labels:
            params["labels"] = labels

        all_issues = []
        total_pages = 0

        self.logger.info(
            f"üîç Buscando issues ({state}) do reposit√≥rio {self.owner}/{self.repo}"
        )

        while True:
            issues = self._make_request(
                url,
                params=params,
                error_message=f"Erro ao obter issues da p√°gina {params['page']}",
            )

            if not issues or len(issues) == 0:
                break

            # Filtra para remover PRs (a API do GitHub retorna PRs como issues)
            filtered_issues = [issue for issue in issues if "pull_request" not in issue]
            all_issues.extend(filtered_issues)

            total_pages = params["page"]

            self.logger.info(
                f"üìã P√°gina {params['page']}: {len(filtered_issues)} issues encontradas"
            )

            # Verificar se tem mais p√°ginas
            if len(issues) < per_page:
                break

            params["page"] += 1

            # Pequena pausa entre requisi√ß√µes para evitar sobrecarga
            time.sleep(0.25)

        self.logger.info(
            f"‚úÖ Total de issues coletadas: {len(all_issues)} em {total_pages} p√°ginas"
        )
        return all_issues

    def fetch_pull_requests(
        self,
        state: str = "all",
        per_page: int = 100,
        sort: str = "created",
        direction: str = "desc",
        base: Optional[str] = None,
    ) -> List[Dict]:
        """
        Obt√©m todos os pull requests de um reposit√≥rio, lidando com pagina√ß√£o.

        Args:
            state: Estado dos PRs (open, closed, all)
            per_page: N√∫mero de itens por p√°gina
            sort: Campo para ordena√ß√£o (created, updated, popularity, long-running)
            direction: Dire√ß√£o da ordena√ß√£o (asc, desc)
            base: Filtrar PRs por branch base

        Returns:
            List[Dict]: Lista com todos os pull requests
        """
        url = f"{self.base_url}/repos/{self.owner}/{self.repo}/pulls"
        params = {
            "state": state,
            "per_page": per_page,
            "page": 1,
            "sort": sort,
            "direction": direction,
        }

        if base:
            params["base"] = base

        all_prs = []
        total_pages = 0

        self.logger.info(
            f"üîç Buscando pull requests ({state}) do reposit√≥rio {self.owner}/{self.repo}"
        )

        while True:
            prs = self._make_request(
                url,
                params=params,
                error_message=f"Erro ao obter PRs da p√°gina {params['page']}",
            )

            if not prs or len(prs) == 0:
                break

            all_prs.extend(prs)
            total_pages = params["page"]

            self.logger.info(f"üìã P√°gina {params['page']}: {len(prs)} PRs encontrados")

            # Verificar se tem mais p√°ginas
            if len(prs) < per_page:
                break

            params["page"] += 1

            # Pequena pausa entre requisi√ß√µes para evitar sobrecarga
            time.sleep(0.25)

        self.logger.info(
            f"‚úÖ Total de pull requests coletados: {len(all_prs)} em {total_pages} p√°ginas"
        )
        return all_prs

    def fetch_pr_details(self, pr_number: int) -> Optional[Dict]:
        """
        Obt√©m detalhes de um pull request espec√≠fico, incluindo reviews e commits.

        Args:
            pr_number: N√∫mero do pull request

        Returns:
            Dict: Dados detalhados do pull request ou None em caso de erro
        """
        # Buscar dados b√°sicos do PR
        pr_url = f"{self.base_url}/repos/{self.owner}/{self.repo}/pulls/{pr_number}"
        pr_data = self._make_request(
            pr_url, error_message=f"Erro ao obter PR #{pr_number}"
        )

        if not pr_data:
            return None

        # Buscar commits do PR
        commits_url = f"{pr_url}/commits"
        commits = self._make_request(
            commits_url, error_message=f"Erro ao obter commits do PR #{pr_number}"
        )

        # Buscar reviews do PR
        reviews_url = f"{pr_url}/reviews"
        reviews = self._make_request(
            reviews_url, error_message=f"Erro ao obter reviews do PR #{pr_number}"
        )

        # Agregar dados
        pr_data["commits_data"] = commits or []
        pr_data["reviews_data"] = reviews or []

        return pr_data

    def fetch_commits(
        self,
        per_page: int = 100,
        since: Optional[str] = None,
        until: Optional[str] = None,
        path: Optional[str] = None,
        author: Optional[str] = None,
    ) -> List[Dict]:
        """
        Obt√©m todos os commits de um reposit√≥rio, lidando com pagina√ß√£o.

        Args:
            per_page: N√∫mero de itens por p√°gina
            since: Data ISO 8601 para filtrar commits a partir desta data (YYYY-MM-DDTHH:MM:SSZ)
            until: Data ISO 8601 para filtrar commits at√© esta data (YYYY-MM-DDTHH:MM:SSZ)
            path: Filtra commits que modificaram arquivos neste caminho
            author: Filtra commits por autor (username GitHub)

        Returns:
            List[Dict]: Lista com todos os commits
        """
        url = f"{self.base_url}/repos/{self.owner}/{self.repo}/commits"
        params = {"per_page": per_page, "page": 1}

        if since:
            params["since"] = since

        if until:
            params["until"] = until

        if path:
            params["path"] = path

        if author:
            params["author"] = author

        all_commits = []
        total_pages = 0

        self.logger.info(f"üîç Buscando commits do reposit√≥rio {self.owner}/{self.repo}")

        while True:
            commits = self._make_request(
                url,
                params=params,
                error_message=f"Erro ao obter commits da p√°gina {params['page']}",
            )

            if not commits or len(commits) == 0:
                break

            all_commits.extend(commits)
            total_pages = params["page"]

            self.logger.info(
                f"üìã P√°gina {params['page']}: {len(commits)} commits encontrados"
            )

            # Verificar se tem mais p√°ginas
            if len(commits) < per_page:
                break

            params["page"] += 1

            # Pequena pausa entre requisi√ß√µes para evitar sobrecarga
            time.sleep(0.25)

        self.logger.info(
            f"‚úÖ Total de commits coletados: {len(all_commits)} em {total_pages} p√°ginas"
        )
        return all_commits

    def fetch_commit_details(self, commit_sha: str) -> Optional[Dict]:
        """
        Obt√©m detalhes de um commit espec√≠fico, incluindo altera√ß√µes.

        Args:
            commit_sha: SHA do commit

        Returns:
            Dict: Dados detalhados do commit ou None em caso de erro
        """
        url = f"{self.base_url}/repos/{self.owner}/{self.repo}/commits/{commit_sha}"
        return self._make_request(
            url, error_message=f"Erro ao obter detalhes do commit {commit_sha}"
        )

    def search_repositories(
        self, query: str, sort: str = "stars", order: str = "desc", per_page: int = 100
    ) -> List[Dict]:
        """
        Pesquisa reposit√≥rios no GitHub.

        Args:
            query: Query de pesquisa
            sort: Campo para ordena√ß√£o (stars, forks, updated)
            order: Dire√ß√£o da ordena√ß√£o (asc, desc)
            per_page: Itens por p√°gina

        Returns:
            List[Dict]: Lista de reposit√≥rios encontrados
        """
        url = f"{self.base_url}/search/repositories"
        params = {
            "q": query,
            "sort": sort,
            "order": order,
            "per_page": per_page,
            "page": 1,
        }

        all_repos = []
        total_count = 0

        while True:
            result = self._make_request(
                url,
                params=params,
                error_message=f"Erro na pesquisa de reposit√≥rios p√°gina {params['page']}",
            )

            if not result or "items" not in result:
                break

            if params["page"] == 1:
                total_count = result.get("total_count", 0)
                self.logger.info(f"üìä Total de resultados encontrados: {total_count}")

            items = result["items"]
            all_repos.extend(items)

            # Verificar se tem mais p√°ginas
            if len(items) < per_page or len(all_repos) >= total_count:
                break

            params["page"] += 1

            # Pequena pausa entre requisi√ß√µes para evitar sobrecarga
            time.sleep(0.5)  # Search API tem limites mais restritos

        return all_repos

    def fetch_code_files(
        self,
        path: str = "",
        ref: str = "main",
        recursive: bool = True,
        file_extensions: Optional[List[str]] = None,
        exclude_dirs: Optional[List[str]] = None,
        max_file_size: int = 1024 * 1024,  # 1MB por padr√£o
    ) -> List[Dict]:
        """
        Obt√©m arquivos de c√≥digo de um reposit√≥rio, com op√ß√µes de filtragem avan√ßada.

        Args:
            path: Caminho base dentro do reposit√≥rio
            ref: Branch ou commit SHA para buscar os arquivos
            recursive: Se deve buscar arquivos em subdiret√≥rios recursivamente
            file_extensions: Lista de extens√µes de arquivo para filtrar (.py, .js, etc)
            exclude_dirs: Lista de diret√≥rios para excluir da busca
            max_file_size: Tamanho m√°ximo do arquivo em bytes para baixar

        Returns:
            List[Dict]: Lista com todos os arquivos de c√≥digo e seus conte√∫dos
        """
        if exclude_dirs is None:
            exclude_dirs = [
                ".git",
                "node_modules",
                "venv",
                "__pycache__",
                "build",
                "dist",
            ]

        if file_extensions is None:
            file_extensions = [
                ".py",
                ".js",
                ".java",
                ".cpp",
                ".h",
                ".c",
                ".ts",
                ".go",
                ".rb",
            ]

        self.logger.info(
            f"üîç Buscando arquivos de c√≥digo em {self.owner}/{self.repo}/{path} (ref: {ref})"
        )

        # Obter a estrutura de arquivos usando o endpoint contents
        url = f"{self.base_url}/repos/{self.owner}/{self.repo}/contents/{path}"
        params = {"ref": ref}

        contents = self._make_request(
            url,
            params=params,
            error_message=f"Erro ao obter estrutura de arquivos em {path}",
        )

        if not contents:
            self.logger.warning(
                f"‚ùå N√£o foi poss√≠vel obter conte√∫do do caminho: {path}"
            )
            return []

        all_files = []

        # Processar cada item no diret√≥rio
        for item in contents:
            item_path = item["path"]
            item_type = item["type"]
            item_name = item["name"]

            # Pular diret√≥rios exclu√≠dos
            if item_type == "dir" and item_name in exclude_dirs:
                self.logger.debug(f"‚è© Pulando diret√≥rio exclu√≠do: {item_path}")
                continue

            # Processar subdiret√≥rios recursivamente
            if item_type == "dir" and recursive:
                self.logger.debug(f"üìÇ Explorando subdiret√≥rio: {item_path}")
                subdir_files = self.fetch_code_files(
                    path=item_path,
                    ref=ref,
                    recursive=recursive,
                    file_extensions=file_extensions,
                    exclude_dirs=exclude_dirs,
                    max_file_size=max_file_size,
                )
                all_files.extend(subdir_files)

            # Processar arquivos de c√≥digo
            elif item_type == "file":
                # Verificar extens√£o do arquivo
                if file_extensions and not any(
                    item_name.endswith(ext) for ext in file_extensions
                ):
                    self.logger.debug(
                        f"‚è© Arquivo ignorado (extens√£o n√£o corresponde): {item_path}"
                    )
                    continue

                # Verificar tamanho do arquivo
                if item["size"] > max_file_size:
                    self.logger.warning(
                        f"‚è© Arquivo muito grande, pulando: {item_path} ({item['size']/1024:.1f} KB)"
                    )
                    continue

                # Obter conte√∫do do arquivo (j√° vem em base64 da API do GitHub)
                file_content = self._make_request(
                    item["url"],
                    error_message=f"Erro ao baixar conte√∫do do arquivo {item_path}",
                    use_cache=True,
                )

                if file_content and "content" in file_content:
                    try:
                        # O conte√∫do vem em base64 e precisa ser decodificado
                        import base64

                        content = base64.b64decode(
                            file_content["content"].replace("\n", "")
                        ).decode("utf-8")

                        file_data = {
                            "path": item_path,
                            "name": item_name,
                            "content": content,
                            "size": item["size"],
                            "sha": item["sha"],
                            "url": item["html_url"],
                        }

                        all_files.append(file_data)
                        self.logger.debug(f"‚úÖ Arquivo processado: {item_path}")
                    except Exception as e:
                        self.logger.error(
                            f"‚ùå Erro ao decodificar arquivo {item_path}: {str(e)}"
                        )

        self.logger.info(
            f"‚úÖ Total de {len(all_files)} arquivos de c√≥digo coletados em {path}"
        )
        return all_files
